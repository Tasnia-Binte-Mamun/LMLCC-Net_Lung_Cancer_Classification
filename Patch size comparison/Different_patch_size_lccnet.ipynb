{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution3D,Conv3D,Add,Concatenate, MaxPool3D,LeakyReLU,add, Convolution2D,BatchNormalization,AveragePooling3D, GlobalAveragePooling3D, ZeroPadding3D\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import operator\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from scipy.ndimage import rotate\n",
    "import random\n",
    "import nibabel as nib\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from matplotlib import animation, rc\n",
    "from scipy.ndimage import rotate\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import KFold, StratifiedKFold,train_test_split\n",
    "from scipy.ndimage import zoom\n",
    "from matplotlib.patches import PathPatch, Rectangle\n",
    "from IPython.display import HTML\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "print(\"Using GPU:\", gpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lidc load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luna_df = pd.read_excel('/home/m-health/TBM/Lung/final_Luna16.xlsx')\n",
    "column_luna = 'malignancy'\n",
    "luna_series=luna_df['Series Uid'].tolist()\n",
    "\n",
    "luna_folder = '/home/m-health/TBM/Lung/Luna_16_hu_from_crop_64'  \n",
    "luna = []\n",
    "for i in luna_series:\n",
    "    nii_file_path = os.path.join(luna_folder, f'{i}.nii')  # Assuming NII files have the extension '.nii'\n",
    "    \n",
    "    if os.path.exists(nii_file_path):\n",
    "        luna.append(i)\n",
    "luna=np.array(luna)\n",
    "\n",
    "print(luna.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify the path to the Excel file\n",
    "file_path = '/home/m-health/TBM/Lung/test_initialtrain_vai_set.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "z_train = df.iloc[:, 2]  \n",
    "\n",
    "z_test_fixed = df.iloc[:, 0]  \n",
    "nan_count = z_test_fixed.isna().sum()\n",
    "z_test = z_test_fixed.dropna()\n",
    "\n",
    "z_val_fixed = df.iloc[:, 1]  \n",
    "nan_count_val = z_val_fixed.isna().sum()\n",
    "z_val = z_val_fixed.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function for rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS= 8 # batch size\n",
    "E = 200 #epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_rotate(x):\n",
    "\n",
    "    rotated_slices = []\n",
    "    for slice_idx in range(x.shape[2]):\n",
    "        slice_data = x[:, :, slice_idx]  # Extract a single slice\n",
    "        rotated_slice = rotate(slice_data, angle=angles[j], reshape=False, mode='nearest')\n",
    "        rotated_slices.append(rotated_slice)\n",
    "\n",
    "    # Create a new NIfTI image from the rotated slices\n",
    "    rotated_img_data = np.stack(rotated_slices, axis=-1)\n",
    "    # Create a new NIfTI image using the header information from the original image\n",
    "    rotated_nifti = nib.Nifti1Image(rotated_img_data, img.affine)\n",
    "    \n",
    "    return rotated_nifti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "\n",
    "\n",
    "\n",
    "datas_test = []\n",
    "labels_test = []\n",
    "scans_test = []\n",
    "\n",
    "datas_train = []\n",
    "labels_train = []\n",
    "scans_train = []\n",
    "\n",
    "datas_val = []\n",
    "labels_val = []\n",
    "scans_val = []\n",
    "\n",
    "\n",
    "for i in z_train:\n",
    "    lab = luna_df.loc[luna_df['Series Uid'] == i, 'malignancy'].values[0]\n",
    "    img_files = glob.glob(os.path.join(luna_folder, f\"{i}.nii\"))\n",
    "    for img_file in img_files:\n",
    "        img = nib.load(img_file)\n",
    "        data = img.get_fdata()\n",
    "        for j in range(len(angles)):\n",
    "            rotated_nifti = aug_rotate(data)\n",
    "            datas_train.append(rotated_nifti.get_fdata())\n",
    "            labels_train.append(lab)\n",
    "                    \n",
    "for i in z_val:\n",
    "    lab = luna_df.loc[luna_df['Series Uid'] == i, 'malignancy'].values[0]\n",
    "    img_files = glob.glob(os.path.join(luna_folder, f\"{i}.nii\"))\n",
    "    for img_file in img_files:\n",
    "        img = nib.load(img_file)\n",
    "        data = img.get_fdata()\n",
    "        datas_val.append(data)\n",
    "        labels_val.append(lab)\n",
    "\n",
    "for i in z_test:\n",
    "    lab = luna_df.loc[luna_df['Series Uid'] == i, 'malignancy'].values[0]\n",
    "    img_files = glob.glob(os.path.join(luna_folder, f\"{i}.nii\"))\n",
    "    for img_file in img_files:\n",
    "        img = nib.load(img_file)\n",
    "        data = img.get_fdata()\n",
    "        datas_test.append(data)\n",
    "        labels_test.append(lab)\n",
    "            \n",
    "X_train = np.array(datas_train)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(datas_test)\n",
    "y_test = np.array(labels_test)\n",
    "X_val = np.array(datas_val)\n",
    "y_val = np.array(labels_val)\n",
    "\n",
    "\n",
    "X_train = np.array(datas_train)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(datas_test)\n",
    "y_test = np.array(labels_test)\n",
    "X_val = np.array(datas_val)\n",
    "y_val = np.array(labels_val)\n",
    "\n",
    "print(z_train.shape, z_test.shape, z_val.shape)\n",
    "print(X_train.shape, X_test.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with LCCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcc():\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(32,(3,3,3), activation='relu', kernel_initializer='he_uniform', padding='same',input_shape=(48,48,48,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(32, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool3D((2, 2,2)))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Conv3D(64, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(64, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool3D((2, 2,2)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool3D((2, 2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool3D((2, 2,2)))\n",
    "\n",
    "    model.add(Conv3D(256, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(256, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(MaxPool3D((2, 2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(8, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='he_uniform'));\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for patch size 48* 48 *48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/m-health/TBM/Lung/\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Build & compile model\n",
    "# ------------------------------------------------------------\n",
    "with strategy.scope():\n",
    "    model = lcc()\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Callbacks\n",
    "# ------------------------------------------------------------\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.001,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(save_dir, \"lcc_net.h5\"),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Train\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "BS = BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=E,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[lr_callback, checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Load best model & evaluate\n",
    "# ------------------------------------------------------------\n",
    "best_model = tf.keras.models.load_model(os.path.join(save_dir, \"lcc_net.h5\"))\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) Predictions\n",
    "# ------------------------------------------------------------\n",
    "predictions = best_model.predict(X_test)\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "class_names = ['benign', 'malignant']\n",
    "print(classification_report(y_test, predicted_classes, target_names=class_names, digits=5))\n",
    "\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9) Confusion Matrix\n",
    "# ------------------------------------------------------------\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print(f'Loss: {loss*100:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for patch size 64* 64 *64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcc():\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(32,(3,3,3), activation='relu', kernel_initializer='he_uniform', padding='same',input_shape=(64,64,64,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(32, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool3D((2, 2,2)))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Conv3D(64, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(64, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool3D((2, 2,2)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool3D((2, 2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(128, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool3D((2, 2,2)))\n",
    "\n",
    "    model.add(Conv3D(256, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv3D(256, (3, 3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(MaxPool3D((2, 2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(8, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='he_uniform'));\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/m-health/TBM/Lung/\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Build & compile model\n",
    "# ------------------------------------------------------------\n",
    "with strategy.scope():\n",
    "    model = lcc()\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Callbacks\n",
    "# ------------------------------------------------------------\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.001,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(save_dir, \"lcc_net_64.h5\"),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Train\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=E,\n",
    "    batch_size=BS,\n",
    "    callbacks=[lr_callback, checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Load best model & evaluate\n",
    "# ------------------------------------------------------------\n",
    "best_model = tf.keras.models.load_model(os.path.join(save_dir, \"lcc_net_64.h5\"))\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) Predictions\n",
    "# ------------------------------------------------------------\n",
    "predictions = best_model.predict(X_test)\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "class_names = ['benign', 'malignant']\n",
    "print(classification_report(y_test, predicted_classes, target_names=class_names, digits=5))\n",
    "\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9) Confusion Matrix\n",
    "# ------------------------------------------------------------\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print(f'Loss: {loss*100:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------------\n",
    "# # 10) Plot & Save Misclassified 3D Volumes (Show Middle Slice)\n",
    "# # ------------------------------------------------------------\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# wrong_pred_dir = os.path.join(save_dir, \"wrong_predictions\")\n",
    "# os.makedirs(wrong_pred_dir, exist_ok=True)\n",
    "\n",
    "# wrong_indices = np.where(predicted_classes.flatten() != y_test.flatten())[0]\n",
    "# print(f\"Total misclassified samples: {len(wrong_indices)}\")\n",
    "\n",
    "# for idx in wrong_indices:\n",
    "#     vol = X_test[idx]  # shape: (64,64,64) or (64,64,64,1)\n",
    "\n",
    "#     # ---- Handle channel dimension if present ----\n",
    "#     if vol.ndim == 4 and vol.shape[-1] == 1:\n",
    "#         vol = vol[..., 0]  # make it (64,64,64)\n",
    "\n",
    "#     # ---- Extract the central slice ----\n",
    "#     mid_slice = vol[:, :, vol.shape[2] // 2]\n",
    "\n",
    "#     true_label = y_test[idx]\n",
    "#     pred_label = predicted_classes[idx][0]\n",
    "#     pred_prob = predictions[idx][0]\n",
    "\n",
    "#     plt.figure(figsize=(4,4))\n",
    "#     plt.imshow(mid_slice, cmap='gray')\n",
    "#     plt.title(\n",
    "#         f\"True: {class_names[int(true_label)]}\\n\"\n",
    "#         f\"Pred: {class_names[int(pred_label)]} (p={pred_prob:.3f})\"\n",
    "#     )\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     save_path = os.path.join(wrong_pred_dir, f\"wrong_{idx}.png\")\n",
    "#     plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "#     plt.close()\n",
    "\n",
    "# print(f\"Saved misclassified CT slice images to: {wrong_pred_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# # Plot & Save Correctly Classified 3D Volumes (Middle Slice)\n",
    "# # ------------------------------------------------------------\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# correct_pred_dir = os.path.join(save_dir, \"correct_predictions\")\n",
    "# os.makedirs(correct_pred_dir, exist_ok=True)\n",
    "\n",
    "# # Option: limit how many to save (set to None to save all)\n",
    "# MAX_SAVE = None  # e.g., 200 or None\n",
    "\n",
    "# # Find indices where prediction == ground truth\n",
    "# correct_indices = np.where(predicted_classes.flatten() == y_test.flatten())[0]\n",
    "# print(f\"Total correctly-classified samples: {len(correct_indices)}\")\n",
    "\n",
    "# saved = 0\n",
    "# for i, idx in enumerate(correct_indices):\n",
    "#     if (MAX_SAVE is not None) and (saved >= MAX_SAVE):\n",
    "#         break\n",
    "\n",
    "#     vol = X_test[idx]  # e.g., (64,64,64) or (64,64,64,1)\n",
    "\n",
    "#     # Remove singleton channel if present\n",
    "#     if vol.ndim == 4 and vol.shape[-1] == 1:\n",
    "#         vol = vol[..., 0]\n",
    "\n",
    "#     # Defensive: ensure vol is 3D\n",
    "#     if vol.ndim != 3:\n",
    "#         # try to squeeze otherwise skip\n",
    "#         vol = np.squeeze(vol)\n",
    "#         if vol.ndim != 3:\n",
    "#             print(f\"Skipping index {idx}: unexpected shape {X_test[idx].shape}\")\n",
    "#             continue\n",
    "\n",
    "#     # Extract middle axial slice\n",
    "#     mid_slice = vol[:, :, vol.shape[2] // 2]\n",
    "\n",
    "#     true_label = int(y_test[idx])\n",
    "#     pred_label = int(predicted_classes[idx].flatten()[0])\n",
    "#     pred_prob = float(predictions[idx].flatten()[0])\n",
    "\n",
    "#     plt.figure(figsize=(4,4))\n",
    "#     plt.imshow(mid_slice, cmap=\"gray\")\n",
    "#     plt.title(\n",
    "#         f\"True: {class_names[true_label]}\\n\"\n",
    "#         f\"Pred: {class_names[pred_label]} (p={pred_prob:.3f})\",\n",
    "#         fontsize=10\n",
    "#     )\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     fname = f\"correct_idx{idx}_true-{class_names[true_label]}_pred-{class_names[pred_label]}_p{pred_prob:.3f}.png\"\n",
    "#     save_path = os.path.join(correct_pred_dir, fname)\n",
    "#     plt.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
    "#     plt.close()\n",
    "\n",
    "#     saved += 1\n",
    "\n",
    "# print(f\"Saved {saved} correctly-classified sample plots to: {correct_pred_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 3974254,
     "sourceId": 7321302,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5536429,
     "sourceId": 9253492,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30628,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
